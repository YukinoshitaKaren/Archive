<i>Title</i>: [Cross-Lingual Knowledge Editing in Large Language Models ](https://arxiv.org/abs/2309.08952) **ACL 2024 main conference**

<i>Author</i>: Jiaan Wang♠∗, Yunlong Liang♢, Zengkui Sun♢, Yuxuan Cao♣, Jiarong Xu<br>

Fudan University ♡Pattern Recognition Center, WeChat AI, Tencent Inc, China ♢Beijing Jiaotong University ♣Zhejiang University

<i>Main ideas</i>: 涵盖不同范式的各种知识编辑方法进行英语编辑，并评估它们在汉语中的表现，反之亦然。评估包括四个方面，即可靠性、通用性、局部性和可移植性。此外，本文分析了编辑后的模型的不一致行为，并讨论了它们的具体挑战

1. 不同语言之间的语言建模差异影响知识编辑的效率
2. 现有的知识编辑方法仍然很难在多语言LLM中将编辑后的知识从一种语言转移到另一种语言
3. 在编辑一种语言的LLM时，其他语言的局部性也可能受到影响
4. 只有IKE方法有可移植性，但是IKE方法的局部性较差
5. 模型存在language mismatch现象

![2024-6-18-1](C:\Users\36475\Desktop\Archive\pic\2024-6-18-1.png)

***

<i>Title</i>: [Journey to the Center of the Knowledge Neurons: Discoveries of Language-Independent Knowledge Neurons and Degenerate Knowledge Neurons](https://export.arxiv.org/pdf/2308.13198v1.pdf)  **AAAI 2024**<br>
<i>Author</i>: Yuheng Chen, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao

School of Artificial Intelligence, University of Chinese Academy of Sciences

Main ideas: 本文主要提出了两个发现，（1）语言无关神经元（以超越语言的形式存储知识）、（2）退化知识神经元（一些知识神经元存储相同的事实知识，模型需要激活至少一个神经元才能正确表达事实）

<img src="C:\Users\36475\Desktop\Archive\pic\2024-6-18-2.png" alt="2024-6-18-1" style="zoom:33%;" />

***

<i>Title</i>: [Language Representation Projection: Can We Transfer Factual Knowledge across Languages in Multilingual Language Models?]([2311.03788.pdf (arxiv.org)](https://arxiv.org/pdf/2311.03788.pdf))  **EMNLP 2023**<br>
<i>Author</i>: Shaoyang Xu, Junzhuo Li and Deyi Xiong

School of New Media and Communication, Tianjin University<br>
<i>Main ideas</i>: 高资源和低资源的语言在事实知识的探究上存在差距，本文提出了LRP2，通过将非英语表示转化为英语等价物。之后再恢复的方法，实现语言表达的转换。LRP2提升了事实知识检索的精度，提高了语言之间的可转移性

<img src="C:\Users\36475\Desktop\Archive\pic\2024-6-18-3.png" alt="2024-6-18-1" style="zoom:38%;" />



***

<i>Title</i>: [Multilingual LAMA: Investigating Knowledge in Multilingual Pretrained Language Models](https://export.arxiv.org/pdf/2102.00894.pdf)  **EACL 2021**<br>
<i>Author</i>: Nora Kassner, Philipp Dufter, Hinrich Sch  ̈utze<br>
<i>Main ideas</i>:提出了mLAMA数据集

1. mBERT的预测结果过于依赖语言
   1. 会产生特定于语言的偏差Bias
   4. mBERT 不是以独立于语言的方式存储实体知识

***

<i>Title</i>: [X-FACTR: Multilingual Factual Knowledge Retrieval from Pretrained Language Models](https://aclanthology.org/2020.emnlp-main.479.pdf)    **EMNLP 2020**<br>
<i>Author</i>: Zhengbao Jiang†, Antonios Anastasopoulos♣,∗, Jun Araki‡, Haibo Ding‡, Graham Neubig†

†Languages Technologies Institute, Carnegie Mellon University ♣Department of Computer Science, George Mason University<br>
<i>Main ideas</i>:提出了一个新的多语言基准，跨语言事实检索基准(X-FACTR)，其中包含23种语言。探测方法由单词实体扩展到多词实体，并开发了多种解码算法。最后本文提出了一种code-switching-based方法，提高多语言LM获取知识的能力

***

<i>Title</i>: [Language Anisotropic Cross-Lingual Model Editing](https://export.arxiv.org/pdf/2205.12677v2.pdf)  **ACL (Findings) 2023**<br>
<i>Author</i>: Yang Xu Yutai Hou Wanxiang Che Min Zhang； Harbin Institute of Technology<br>
<i>Main ideas</i>:定义了跨语言模型编辑任务和相应的指标。提出了一个框架，使用并行语料库自然地将单语模型编辑方法应用于跨语言场景。此外，提出了语言各向异性编辑，通过放大每种语言的不同参数子集来改进跨语言编辑。

<img src="C:\Users\36475\Desktop\Archive\pic\2023-6-18-5.png" alt="task prompts.png" style="zoom: 50%;" />

***

<i>Title</i>: [CROSS-LINGUAL ABILITY OF MULTILINGUAL BERT: AN EMPIRICAL STUDY](https://arxiv.org/abs/1912.07840)  **ICLR 2020 Conference**<br>
<i>Author</i>: Karthikeyan K, Zihan Wang, Stephen Mayhew, Dan Roth ；Indian Institute of Technology Kanpur<br>
<i>Main ideas</i>: 研究M-BERT中，语言的**语言属性、模型的体系结构和学习目标**的影响

1. **B-BERT训练数据中的词块重叠量对性能改进几乎没有贡献**
2. **深度和参数总数对B-BERT的单语和跨语成绩都是至关重要的，而多头注意并不是一个显著的因素**
3. **跨语言能力受益于结构相似性**
4. **单字频率本身并不能为跨语言学习提供足够的信息**

***

<i>Title</i>: [Who's Harry Potter? Approximate Unlearning in LLMs](https://arxiv.org/pdf/2310.02238.pdf)   ArXiv *4 Oct 2023*<br>
<i>Author</i>: Ronen Eldan and Mark Russinovich  ；Microsoft Research<br>
<i>Main ideas</i>: 提出了一种有效的遗忘技术

1. 训练一个强化模型，识别与希望遗忘目标最相关的tokens
2. 将目标数据中的特殊表达式替换为通用表达式，并利用模型自己的预测为每个token生成替代标签。这些标签旨在近似尚未在目标数据上训练的模型的下一个标记预测
3. 在这些替代标签上对模型进行微调，无论何时提示其上下文，这都会有效地从模型的内存中删除原始文本

***

<i>Title</i>: [Learning to Edit: Aligning LLMs with Knowledge Editing](https://arxiv.org/pdf/2310.02238.pdf)    **ACL 2024**<br>
<i>Author</i>: Yuxin Jiang1,2∗, Yufei Wang3, Chuhan Wu3, Wanjun Zhong3, Xingshan Zeng3, Jiahui Gao3, Liangyou Li3, Xin Jiang3, Lifeng Shang3, Ruiming Tang3, Qun Liu3, Wei Wang1,2（港科技，华为<br>
<i>Main ideas</i>: （i）对齐阶段，在并行数据集上微调LLM，In-Scope编辑，同时保留Out-of-Scope能力；（ii）推理阶段，该阶段采用基于检索的机制(受RAG启发)进行知识编辑

<img src="C:\Users\36475\Desktop\Archive\pic\2024-6-18-4.png" alt="2024-6-18-1" style="zoom:38%;" />

***

<i>Title</i>: [Cross-lingual Editing in Multilingual Language Models](https://arxiv.org/pdf/2310.02238.pdf)  **EACL 2024**<br>
<i>Author</i>: Himanshu Beniwal†∗, Kowsik Nandagopan D∗, Mayank Singh（印度理工<br>

<i>Main ideas</i>: 创建了一个数据集，包含俩大语系和6种语言，主要在MEND和FT方法上做了跨语言的测试，对于FT，尝试了很多微调语言和测试语言不同的排列组合

1. 事实知识可能存储在不同层
2. 微调语言是否对编辑有影响？是否将知识转移到了别的层
3. 不同的体系结构将事实知识存储在不同的位置
4. 初始微调显著影响仅编码器模型的 **局部性** 分数，而仅解码器模型没有观察到这一观察结果

***

<i>Title</i>: [Cross-Lingual Consistency of Factual Knowledge in Multilingual Language Models](https://arxiv.org/pdf/2310.02238.pdf)  **EMNLP2023**<br>
<i>Author</i>: Jirui Qi 1, Raquel Fernández2, Arianna Bisazza（阿姆斯特丹大学<br>

<i>Main ideas</i>: 研究了各种多语言 PLM 中事实知识的跨语言一致性 (CLC)，没有衡量PLM在每种语言中编码的事实知识的数量，而是关注其在**不同语言之间的一致性**。并且，提出了一种基于排名的一致性 (RankC) 度量（所有的问题和候选答案 都应该有所有语言的版本

1. 事实知识可能主要以一种相当**肤浅**的方式渗透到语言中（通过共享一些**子词嵌入**），相反，即使语言是相关的，也可能在缺乏这些**锚**的情况下受到阻碍
2. subword vocabularies 如果进一步扩大，那么可能一致性会更加的差
3. 通过模型编辑（ROME）插入到语言X中的新颖事实更有可能传播到具有较高CLC分数的语言中。

***

<i>Title</i>: [KNOWLEDGE CARD: FILLING LLMS' KNOWLEDGE GAPS WITH PLUG-IN SPECIALIZED LANGUAGE MODELS](https://arxiv.org/pdf/2310.02238.pdf)  **ICLR 2024, oral**<br>
<i>Author</i>: Shangbin Feng1 Weijia Shi1

University of Washington ；Xi'an Jiaotong University<br>

<i>Main ideas</i>: **KNOWLEDGE CARD** 是一种模块化框架，用于将新的事实和相关知识插入到通用 LLM 中。作者训练了专门领域的小模型，作为knowledge cards，在模型推理时辅助模型生成（类似RAG）

***

<i>Title</i>: [Language Modeling with Editable External Knowledge](https://arxiv.org/pdf/2310.02238.pdf)  **ArXiv 17 Jun 2024**<br>
<i>Author</i>: Belinda Z. Li1, Emmy Liu2, Alexis Ross1, Abbas Zeitoun1, Graham Neubig2, Jacob Andreas1

Massachusetts Institute of Technology, CSAIL2 Carnegie Mellon University, Language Technologies Institute<br>

<i>Main ideas</i>: 提出了ERASE方法，重点是在更新数据库的同时，对以往那些过时的文档进行删除或者修改。同时构建了随着时间变化，答案也有所不同的数据集 CLARK

<img src="C:\Users\36475\Desktop\Archive\pic\2024-6-18-5.png" alt="2024-6-18-1" style="zoom:38%;" />

***

**<i>Title</i>: [MEMLA: Enhancing Multilingual Knowledge Editing with Neuron-Masked Low-Rank Adaptation](https://arxiv.org/pdf/2310.02238.pdf)  **ArXiv 17 Jun 2024**<br>
<i>Author</i>: Jiakuan Xie1,2, Pengfei Cao1,2, Yuheng Chen1,2, Yubo Chen1,2, Kang Liu1,2, Jun Zhao1,2

University of Chinese Academy of Sciences<br>

<i>Main ideas</i>: 提出MEMLA方法，进行多语种知识编辑。首先利用积分梯度来找到语言无关神经元和特定语言神经元，然后利用LORA进行参数修改，是一种类似ROMA的方法，最终结果是优于ROMA和MIMIT这类方法，但是全面不如IKE，作者认为IKE并没有改变模型的内部知识，应该视为一个理论上限。

<img src="C:\Users\36475\Desktop\Archive\pic\2024-6-19-1.png" alt="2024-6-18-1" style="zoom:38%;" />

***

**<i>Title</i>: [In-Context Editing: Learning Knowledge from Self-Induced Distributions](https://arxiv.org/pdf/2310.02238.pdf)  **ArXiv 17 Jun 2024**<br>
<i>Author</i>: Siyuan Qi1† Bangcheng Yang1 Kailin Jiang1 Xiaobo Wang1Jiaqi Li1 Yifan Zhong1,2 Yaodong Yang1,2 Zilong Zheng

State Key Laboratory of General Artificial Intelligence, BIGAI Peking University<br>

<i>Main ideas</i>: 提出了一种利用上下文微调的ICE方法（动态调整目标分布和使用上下文提示），令我比较惊讶的是该方法在可移植性和局部性上表现比较优秀

<img src="C:\Users\36475\Desktop\Archive\pic\2024-6-20-1.png" alt="2024-6-18-1" style="zoom:38%;" />

<img src="C:\Users\36475\Desktop\Archive\pic\2024-6-20-2.png" alt="2024-6-18-1" style="zoom:38%;" />

***

